# Click-Through Rate (CTR) Prediction with Feature Engineering & XGBoost

**Predicting ad clicks using advanced feature engineering and gradient boosting**

[![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![scikit-learn](https://img.shields.io/badge/scikit--learn-1.3+-orange.svg)](https://scikit-learn.org/)
[![XGBoost](https://img.shields.io/badge/XGBoost-2.0+-green.svg)](https://xgboost.readthedocs.io/)
[![License](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

---

## ğŸ“Š Project Overview

Built an end-to-end machine learning system to predict whether users will click on advertisements using real-world data from Kaggle's Avazu CTR competition. Demonstrates advanced feature engineering and compares traditional ML (Logistic Regression) with gradient boosting (XGBoost).

**Business Impact:**
- **75-80% AUC-ROC:** Significantly better than random (50%)
- **40-60% CTR Lift:** By targeting top 10% of predicted users
- **2-3x Efficiency:** Better than random ad targeting
- **Production-Ready:** Scalable feature engineering pipeline

---

## ğŸ¯ Key Features

- **Advanced Feature Engineering:** 30+ features from temporal, behavioral, and interaction patterns
- **Class Imbalance Handling:** 1:5 imbalance using class weights and stratified sampling
- **Model Comparison:** Logistic Regression (baseline) vs XGBoost (production)
- **Business Metrics:** CTR lift, targeting efficiency, ROI analysis

---

## ğŸ“ˆ Results

### **Model Performance**

| Metric | Logistic Regression | XGBoost | Improvement |
|--------|-------------------|---------|-------------|
| AUC-ROC | 0.70-0.75 | 0.75-0.80 | +5-10% |
| Accuracy | 75-80% | 80-85% | +5% |

### **Business Impact**

| Strategy | CTR | Lift | Efficiency |
|----------|-----|------|-----------|
| Random | 17% | 0% | 1.0x |
| Top 10% (XGBoost) | 24-27% | +40-60% | 2.5-3.0x |

---

## ğŸ› ï¸ Technical Stack

- Python, pandas, NumPy
- scikit-learn, XGBoost
- Feature Engineering: Frequency encoding, interactions, aggregations
- Evaluation: AUC-ROC, Precision-Recall, Log Loss

---

## ğŸš€ Quick Start

```bash
git clone https://github.com/imadniz/ctr-prediction.git
cd ctr-prediction
pip install -r requirements.txt
jupyter notebook
```

Run notebooks in order: 01 â†’ 02 â†’ 03 â†’ 04

---

## ğŸ’¡ Key Insights

1. **Feature engineering >> Model selection** for CTR prediction
2. Gradient boosting beats linear models consistently
3. Temporal and behavioral features are most predictive
4. Proper imbalance handling is critical

---

## ğŸ“« Contact

**Imad Nizami**  
Email: imadniz96@gmail.com  
LinkedIn: [linkedin.com/in/imadnizami](https://www.linkedin.com/in/imadnizami)  
GitHub: [github.com/imadniz](https://github.com/imadniz)

---

â­ Star this repo if you found it helpful!
